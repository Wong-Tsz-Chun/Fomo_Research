{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6437755-72f0-4650-8200-b383e65efb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from collections import Counter\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import os\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abdb82fd-e707-4f5d-ab1a-9ed32fff2c9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and cleaning data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\asus\\AppData\\Local\\Temp\\ipykernel_2648\\2378663970.py:13: DtypeWarning: Columns (10,11,12,13,14,15,16,17,18,19,20,21,22,23) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.read_csv(file_path, encoding='latin1')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing sentiment analysis...\n",
      "Analyzing keywords...\n",
      "Saving analysis report...\n",
      "Analysis complete. Results saved in FOMO_reports/youtube/\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "output_dir = \"FOMO_reports/youtube\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def clean_and_prepare_data(file_path):\n",
    "    \"\"\"Load and clean data with error handling for UTF-8 encoding\"\"\"\n",
    "    try:\n",
    "        # First attempt with UTF-8 encoding\n",
    "        data = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        try:\n",
    "            # Second attempt with Latin-1 encoding\n",
    "            data = pd.read_csv(file_path, encoding='latin1')\n",
    "        except Exception as e:\n",
    "            # If both attempts fail, try with CP1252 encoding\n",
    "            data = pd.read_csv(file_path, encoding='cp1252')\n",
    "    \n",
    "    # Clean content\n",
    "    data['content'] = data['content'].apply(lambda x: str(x) if pd.notnull(x) else '')\n",
    "    data['content'] = data['content'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "    data['content'] = data['content'].apply(lambda x: re.sub(r'@\\w+', '', x))\n",
    "    data['content'] = data['content'].apply(lambda x: re.sub(r'[^a-zA-Z\\s]', '', x))\n",
    "    data['content'] = data['content'].str.lower()\n",
    "    data['content'] = data['content'].str.strip()\n",
    "    data['content'] = data['content'].replace('', np.nan)\n",
    "    \n",
    "    # Filter comments\n",
    "    word_counts = data['content'].apply(lambda x: len(str(x).split()) if pd.notnull(x) else 0)\n",
    "    data = data[word_counts.between(10, 200)]\n",
    "    return data.dropna(subset=['content'])\n",
    "\n",
    "# Define custom stop words and combine with NLTK stop words\n",
    "nltk_stop_words = set(stopwords.words('english'))\n",
    "custom_stop_words = ['video','ive','really', 'also', 'even', 'know', 'get', 'one', 'even', 'jaiden', \n",
    "                    'im', 'like', 'dont', 'want', 'go', 'yeah', 'oh', 'well', 'okay', 'ya', \n",
    "                    'yep', 'nah', 'nope', 'hmm', 'huh', 'uh', 'um', 'err', 'ah', 'er', 'hey', \n",
    "                    'wow', 'oops', 'ooh', 'whoa', 'haha', 'lol', 'lmao', 'rofl', 'omg', 'wtf', \n",
    "                    'damn', 'dang', 'shit', 'fuck', 'crap', 'ass', 'bitch', 'hell', 'damn', \n",
    "                    'freaking', 'bloody', 'freakin', 'frickin', 'fricking', 'friggin', 'friggen', \n",
    "                    'gosh', 'geez', 'darn', 'dang', 'cuz', 'coz', 'cause', 'cos', 'dunno', \n",
    "                    'gonna', 'gotta', 'wanna', 'gonna', 'kinda', 'sorta', 'coulda', 'shoulda', \n",
    "                    'woulda', 'lemme', 'prolly', 'bruh', 'bro', 'dude', 'man', 'guys', 'mate', \n",
    "                    'mates', 'folks', 'peeps', 'peepz', 'peepol', 'ppl', 'yall']\n",
    "all_stop_words = nltk_stop_words.union(set(custom_stop_words))\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"Process and filter text using NLTK\"\"\"\n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Filter tokens\n",
    "    filtered_tokens = [\n",
    "        word.lower() for word in tokens \n",
    "        if word.lower() not in all_stop_words \n",
    "        and len(word) > 2 \n",
    "        and word.isalpha()\n",
    "    ]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "def analyze_sentiment(data):\n",
    "    \"\"\"Perform sentiment analysis and create plots\"\"\"\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    data['vader_sentiment'] = data['content'].apply(lambda text: analyzer.polarity_scores(text)['compound'])\n",
    "    \n",
    "    # Plot sentiment distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(data['vader_sentiment'], bins=20, color='skyblue', edgecolor='black')\n",
    "    plt.title('Distribution of Compound Sentiment Scores on Youtube')\n",
    "    plt.xlabel('Sentiment Score')\n",
    "    plt.ylabel('Count')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/sentiment_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "def analyze_keywords(data):\n",
    "    \"\"\"Analyze keywords using NLTK and create visualizations\"\"\"\n",
    "    # Process all comments\n",
    "    all_tokens = []\n",
    "    all_text = []\n",
    "    \n",
    "    for comment in data['content']:\n",
    "        processed_tokens = process_text(comment)\n",
    "        all_tokens.extend(processed_tokens)\n",
    "        all_text.append(' '.join(processed_tokens))\n",
    "    \n",
    "    # Word frequency analysis\n",
    "    word_counts = Counter(all_tokens)\n",
    "    \n",
    "    # Plot top 20 keywords\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    top_words = word_counts.most_common(20)\n",
    "    plt.bar(range(20), [count for _, count in top_words], color='skyblue')\n",
    "    plt.xticks(range(20), [word for word, _ in top_words], rotation=45, ha='right')\n",
    "    plt.title(\"Top 20 Keyword Counts (NLTK Filtered)\")\n",
    "    plt.xlabel(\"Keyword\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{output_dir}/top_keywords.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Create wordcloud\n",
    "    # Get the top 40 words and their frequencies\n",
    "    top_40_words = dict(word_counts.most_common(40))\n",
    "    \n",
    "    # Calculate frequency-based font sizes\n",
    "    max_freq = max(top_40_words.values())\n",
    "    min_freq = min(top_40_words.values())\n",
    "    \n",
    "    # Create wordcloud with frequency-based sizing\n",
    "    # Get the top 40 words and their frequencies\n",
    "    top_40_words = dict(word_counts.most_common(40))\n",
    "    \n",
    "    # Create wordcloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=1600,\n",
    "        height=800,\n",
    "        background_color='white',\n",
    "        stopwords=all_stop_words,\n",
    "        max_words=40,\n",
    "        random_state=42,\n",
    "        prefer_horizontal=0.7\n",
    "    ).generate_from_frequencies(top_40_words)\n",
    "    \n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud of Top 40 Keywords', fontsize=20, pad=20)\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.savefig(f'{output_dir}/wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    return word_counts\n",
    "\n",
    "def save_analysis_report(data, word_counts):\n",
    "    \"\"\"Save analysis results to a text file with encoding error handling\"\"\"\n",
    "    try:\n",
    "        with open(f'{output_dir}/analysis_report.txt', 'w', encoding='utf-8') as f:\n",
    "            write_report_content(f, data, word_counts)\n",
    "    except UnicodeEncodeError:\n",
    "        with open(f'{output_dir}/analysis_report.txt', 'w', encoding='cp1252') as f:\n",
    "            write_report_content(f, data, word_counts)\n",
    "\n",
    "def write_report_content(f, data, word_counts):\n",
    "    \"\"\"Helper function to write the actual report content\"\"\"\n",
    "    f.write(\"YouTube Comments Analysis Report\\n\")\n",
    "    f.write(\"==============================\\n\\n\")\n",
    "    \n",
    "    f.write(\"Dataset Statistics:\\n\")\n",
    "    f.write(f\"Total comments analyzed: {len(data)}\\n\")\n",
    "    f.write(f\"Average sentiment score: {data['vader_sentiment'].mean():.3f}\\n\")\n",
    "    f.write(f\"Positive comments: {len(data[data['vader_sentiment'] > 0])} ({len(data[data['vader_sentiment'] > 0])/len(data)*100:.1f}%)\\n\")\n",
    "    f.write(f\"Negative comments: {len(data[data['vader_sentiment'] < 0])} ({len(data[data['vader_sentiment'] < 0])/len(data)*100:.1f}%)\\n\")\n",
    "    f.write(f\"Neutral comments: {len(data[data['vader_sentiment'] == 0])} ({len(data[data['vader_sentiment'] == 0])/len(data)*100:.1f}%)\\n\\n\")\n",
    "    \n",
    "    f.write(\"Sentiment Statistics:\\n\")\n",
    "    f.write(f\"Maximum sentiment score: {data['vader_sentiment'].max():.3f}\\n\")\n",
    "    f.write(f\"Minimum sentiment score: {data['vader_sentiment'].min():.3f}\\n\")\n",
    "    f.write(f\"Median sentiment score: {data['vader_sentiment'].median():.3f}\\n\")\n",
    "    f.write(f\"Standard deviation: {data['vader_sentiment'].std():.3f}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Top 20 Keywords (NLTK Filtered):\\n\")\n",
    "    for word, count in word_counts.most_common(20):\n",
    "        f.write(f\"{word}: {count}\\n\")\n",
    "def main():\n",
    "    # Load and process data\n",
    "    print(\"Loading and cleaning data...\")\n",
    "    data = clean_and_prepare_data('v2_youtube_comments.csv')\n",
    "    \n",
    "    # Perform sentiment analysis\n",
    "    print(\"Performing sentiment analysis...\")\n",
    "    analyze_sentiment(data)\n",
    "    \n",
    "    # Analyze keywords\n",
    "    print(\"Analyzing keywords...\")\n",
    "    word_counts = analyze_keywords(data)\n",
    "    \n",
    "    # Save report\n",
    "    print(\"Saving analysis report...\")\n",
    "    save_analysis_report(data, word_counts)\n",
    "    \n",
    "    print(f\"Analysis complete. Results saved in {output_dir}/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5309b07-7c09-4225-a05c-a50233731608",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
