{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca59daa7-ebff-4214-abf6-fd10e09ce242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from collections import Counter\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from wordcloud import WordCloud\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger_eng')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8c77c90f-6f4d-4304-aa17-a900435acf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis of: v3_reddit_fomo_data.csv\n",
      "\n",
      "Analyzing filtered content...\n",
      "\n",
      "Filtering Statistics:\n",
      "Total words after filtering: 96466\n",
      "Unique words after filtering: 8964\n",
      "\n",
      "Top 20 words after filtering:\n",
      "feel: 1431\n",
      "people: 1261\n",
      "anxiety: 1082\n",
      "time: 972\n",
      "get: 954\n",
      "friends: 950\n",
      "know: 902\n",
      "life: 858\n",
      "want: 768\n",
      "fomo: 747\n",
      "social: 697\n",
      "think: 591\n",
      "make: 504\n",
      "something: 498\n",
      "help: 464\n",
      "feeling: 463\n",
      "way: 440\n",
      "much: 426\n",
      "please: 419\n",
      "see: 400\n",
      "\n",
      "Performing sentiment analysis...\n",
      "\n",
      "Analyzing engagement patterns...\n",
      "\n",
      "Analyzing quarterly post counts...\n",
      "\n",
      "Quarterly Post Counts:\n",
      "2014Q3: 2 posts\n",
      "2015Q1: 14 posts\n",
      "2015Q2: 3 posts\n",
      "2015Q3: 5 posts\n",
      "2015Q4: 19 posts\n",
      "2016Q1: 4 posts\n",
      "2016Q2: 25 posts\n",
      "2016Q3: 1 posts\n",
      "2016Q4: 12 posts\n",
      "2017Q1: 6 posts\n",
      "2017Q3: 4 posts\n",
      "2017Q4: 12 posts\n",
      "2018Q1: 18 posts\n",
      "2018Q2: 8 posts\n",
      "2018Q3: 25 posts\n",
      "2018Q4: 16 posts\n",
      "2019Q1: 18 posts\n",
      "2019Q2: 21 posts\n",
      "2019Q3: 49 posts\n",
      "2019Q4: 12 posts\n",
      "2020Q1: 22 posts\n",
      "2020Q2: 31 posts\n",
      "2020Q3: 62 posts\n",
      "2020Q4: 47 posts\n",
      "2021Q1: 131 posts\n",
      "2021Q2: 71 posts\n",
      "2021Q3: 51 posts\n",
      "2021Q4: 57 posts\n",
      "2022Q1: 66 posts\n",
      "2022Q2: 219 posts\n",
      "2022Q3: 97 posts\n",
      "2022Q4: 118 posts\n",
      "2023Q1: 336 posts\n",
      "2023Q2: 37 posts\n",
      "2023Q3: 92 posts\n",
      "2023Q4: 211 posts\n",
      "2024Q1: 120 posts\n",
      "2024Q2: 194 posts\n",
      "2024Q3: 288 posts\n",
      "2024Q4: 75 posts\n",
      "\n",
      "Analyzing yearly top keywords...\n",
      "\n",
      "Yearly Top 3 Keywords:\n",
      "\n",
      "2014:\n",
      "  crowdsourced: 1\n",
      "  lousy: 1\n",
      "  shirt: 1\n",
      "\n",
      "2015:\n",
      "  feel: 40\n",
      "  people: 38\n",
      "  social: 32\n",
      "\n",
      "2016:\n",
      "  get: 31\n",
      "  know: 25\n",
      "  friends: 25\n",
      "\n",
      "2017:\n",
      "  feel: 14\n",
      "  people: 12\n",
      "  social: 11\n",
      "\n",
      "2018:\n",
      "  feel: 65\n",
      "  anxiety: 43\n",
      "  time: 38\n",
      "\n",
      "2019:\n",
      "  feel: 80\n",
      "  people: 73\n",
      "  time: 53\n",
      "\n",
      "2020:\n",
      "  feel: 101\n",
      "  people: 98\n",
      "  fomo: 94\n",
      "\n",
      "2021:\n",
      "  feel: 242\n",
      "  people: 160\n",
      "  friends: 148\n",
      "\n",
      "2022:\n",
      "  feel: 270\n",
      "  people: 204\n",
      "  friends: 169\n",
      "\n",
      "2023:\n",
      "  anxiety: 287\n",
      "  people: 276\n",
      "  feel: 250\n",
      "\n",
      "2024:\n",
      "  feel: 352\n",
      "  people: 343\n",
      "  anxiety: 278\n",
      "\n",
      "Saving report...\n",
      "\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class FOMOAnalyzer:\n",
    "    def __init__(self, input_file, output_dir=None, report_prefix=None):\n",
    "        \n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        \n",
    "        # Define blacklist words\n",
    "        self.blacklist = {\n",
    "            'http', 'https', 'www', 'com', 'html', 'htm',\n",
    "            'amp', 'rt', 'url', 'href', 'src', 'png', 'jpg',\n",
    "            'jpeg', 'gif', 'pdf', 'xml', 'php', 'asp', 'js',\n",
    "            'css', 'img', 'pic', 'download', 'click', 'link',\n",
    "            'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves',\n",
    "            'you', 'your', 'yours', 'yourself', 'yourselves',\n",
    "            'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself',\n",
    "            'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "            'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those',\n",
    "            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being',\n",
    "            'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing',\n",
    "            'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as',\n",
    "            'even', 'also', 'going', 'things'\n",
    "        }\n",
    "        \n",
    "        # Set input and output parameters\n",
    "        self.input_file = input_file\n",
    "        self.output_dir = output_dir or 'analysis_output'\n",
    "        self.report_prefix = report_prefix or 'fomo_report'\n",
    "        self.img_dir = f'{self.output_dir}/{report_prefix}'\n",
    "        \n",
    "        # Create output directories\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(self.img_dir, exist_ok=True)\n",
    "        \n",
    "        # Read and validate input data\n",
    "        try:\n",
    "            self.df = pd.read_csv(self.input_file, names=['timestamp', 'source', 'content', 'author', 'engagement'])\n",
    "            self.df['timestamp'] = pd.to_datetime(self.df['timestamp'])\n",
    "        except FileNotFoundError:\n",
    "            raise FileNotFoundError(f\"Input file not found: {self.input_file}\")\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error reading input file: {str(e)}\")\n",
    "        \n",
    "        # Initialize report data\n",
    "        self.report_data = {\n",
    "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'input_file': self.input_file,\n",
    "                'total_posts': len(self.df),\n",
    "                'date_range': f\"{self.df['timestamp'].min().strftime('%Y-%m-%d')} to {self.df['timestamp'].max().strftime('%Y-%m-%d')}\",\n",
    "                'engagement_stats': {},\n",
    "                'sentiment_analysis': {},\n",
    "                'keyword_frequency': {},\n",
    "            }\n",
    "\n",
    "    def clean_text(self, text):\n",
    "\n",
    "        # Convert to lowercase\n",
    "        text = str(text).lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "        \n",
    "        # Remove special characters and digits\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # Tokenize using NLTK\n",
    "        tokens = word_tokenize(text)\n",
    "        \n",
    "        # Get NLTK stopwords\n",
    "        nltk_stop_words = set(stopwords.words('english'))\n",
    "        all_stop_words = nltk_stop_words.union(self.blacklist)\n",
    "        \n",
    "        # Part of speech tagging\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        \n",
    "        # Filter words using multiple criteria\n",
    "        filtered_words = []\n",
    "        for word, pos in pos_tags:\n",
    "            if (word.strip() and  # Not empty\n",
    "                word not in all_stop_words and  # Not in stopwords\n",
    "                len(word) > 2 and  # Longer than 2 characters\n",
    "                not any(char.isdigit() for char in word) and  # No digits\n",
    "                pos in ['NN', 'NNS', 'NNP', 'NNPS', 'JJ', 'JJR', 'JJS', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']  # Only meaningful parts of speech\n",
    "                ):\n",
    "                filtered_words.append(word)\n",
    "        \n",
    "        return ' '.join(filtered_words)\n",
    "\n",
    "    def analyze_filtered_content(self):\n",
    "\n",
    "        # Get all words after cleaning\n",
    "        all_words = []\n",
    "        all_text = []  # for wordcloud\n",
    "        for content in self.df['content']:\n",
    "            cleaned_text = self.clean_text(content)\n",
    "            cleaned_words = cleaned_text.split()\n",
    "            all_words.extend(cleaned_words)\n",
    "            all_text.append(cleaned_text)\n",
    "        \n",
    "        # Count word frequencies\n",
    "        word_freq = Counter(all_words)\n",
    "        \n",
    "        # Store top words in report\n",
    "        self.report_data['keyword_frequency'] = dict(word_freq.most_common(20))\n",
    "        \n",
    "        # Create wordcloud\n",
    "        combined_text = ' '.join(all_text)\n",
    "        wordcloud = WordCloud(\n",
    "            width=1600,\n",
    "            height=800,\n",
    "            background_color='white',\n",
    "            max_words=40,\n",
    "            random_state=42,\n",
    "            prefer_horizontal=0.7\n",
    "        ).generate_from_frequencies(dict(word_freq.most_common(40)))\n",
    "        \n",
    "        plt.figure(figsize=(20,10))\n",
    "        plt.imshow(wordcloud, interpolation='bilinear')\n",
    "        plt.axis('off')\n",
    "        plt.title('Word Cloud of Top 40 Keywords', fontsize=20, pad=20)\n",
    "        plt.tight_layout(pad=0)\n",
    "        plt.savefig(f'{self.img_dir}/{self.report_prefix}_wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        \n",
    "        # Print filtering statistics\n",
    "        total_words = len(all_words)\n",
    "        unique_words = len(set(all_words))\n",
    "        \n",
    "        print(\"\\nFiltering Statistics:\")\n",
    "        print(f\"Total words after filtering: {total_words}\")\n",
    "        print(f\"Unique words after filtering: {unique_words}\")\n",
    "        print(\"\\nTop 20 words after filtering:\")\n",
    "        for word, count in word_freq.most_common(20):\n",
    "            print(f\"{word}: {count}\")\n",
    "\n",
    "    def analyze_sentiment(self, text):\n",
    "\n",
    "        sentiment_scores = self.sia.polarity_scores(text)\n",
    "        \n",
    "        # Classify sentiment based on compound score\n",
    "        if sentiment_scores['compound'] >= 0.05:\n",
    "            sentiment_label = \"Positive\"\n",
    "        elif sentiment_scores['compound'] <= -0.05:\n",
    "            sentiment_label = \"Negative\"\n",
    "        else:\n",
    "            sentiment_label = \"Neutral\"\n",
    "            \n",
    "        return sentiment_scores, sentiment_label\n",
    "\n",
    "    def perform_sentiment_analysis(self):\n",
    "\n",
    "        sentiments = []\n",
    "        detailed_sentiments = []\n",
    "        pos_count = 0\n",
    "        neg_count = 0\n",
    "        neu_count = 0\n",
    "        \n",
    "        for content in self.df['content']:\n",
    "            cleaned_content = self.clean_text(content)\n",
    "            sentiment_scores, sentiment_label = self.analyze_sentiment(cleaned_content)\n",
    "            \n",
    "            sentiments.append(sentiment_scores['compound'])\n",
    "            detailed_sentiments.append({\n",
    "                'pos': sentiment_scores['pos'],\n",
    "                'neg': sentiment_scores['neg'],\n",
    "                'neu': sentiment_scores['neu'],\n",
    "                'compound': sentiment_scores['compound'],\n",
    "                'label': sentiment_label\n",
    "            })\n",
    "            \n",
    "            if sentiment_label == \"Positive\":\n",
    "                pos_count += 1\n",
    "            elif sentiment_label == \"Negative\":\n",
    "                neg_count += 1\n",
    "            else:\n",
    "                neu_count += 1\n",
    "\n",
    "        self.df['sentiment'] = sentiments\n",
    "        self.df['detailed_sentiment'] = detailed_sentiments\n",
    "        \n",
    "        # Store sentiment analysis results\n",
    "        self.report_data['sentiment_analysis'] = {\n",
    "            'mean_compound': float(np.mean(sentiments)),\n",
    "            'positive_posts': pos_count,\n",
    "            'negative_posts': neg_count,\n",
    "            'neutral_posts': neu_count,\n",
    "            'sentiment_distribution': {\n",
    "                'positive': pos_count / len(sentiments),\n",
    "                'negative': neg_count / len(sentiments),\n",
    "                'neutral': neu_count / len(sentiments)\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Create visualization\n",
    "        self.visualize_sentiment_analysis(sentiments, pos_count, neg_count, neu_count)\n",
    "\n",
    "\n",
    "    def visualize_sentiment_analysis(self, sentiments, pos_count, neg_count, neu_count):\n",
    "\n",
    "        # Compound score distribution\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.histplot(sentiments, bins=20, color='skyblue', edgecolor='black')\n",
    "        plt.title('Distribution of Compound Sentiment Scores on Reddit')\n",
    "        plt.xlabel('Compound Score')\n",
    "        plt.ylabel('Count')\n",
    "        plt.savefig(f'{self.img_dir}/{self.report_prefix}_sentiment_dist.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Sentiment categories pie chart\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.pie([pos_count, neu_count, neg_count],\n",
    "                labels=['Positive', 'Neutral', 'Negative'],\n",
    "                colors=['green', 'gray', 'red'],\n",
    "                autopct='%1.1f%%')\n",
    "        plt.title('Overall Sentiment Distribution')\n",
    "        plt.savefig(f'{self.img_dir}/{self.report_prefix}_sentiment_pie.png')\n",
    "        plt.close()\n",
    "        \n",
    "    # Quarterly sentiment percentages with line graph and yearly labels\n",
    "        self.df['quarter'] = self.df['timestamp'].dt.to_period('Q')\n",
    "        \n",
    "        # Calculate counts first\n",
    "        quarterly_counts = pd.DataFrame({\n",
    "            'Positive': self.df[self.df['sentiment'] >= 0.05].groupby('quarter').size(),\n",
    "            'Neutral': self.df[(self.df['sentiment'] > -0.05) & (self.df['sentiment'] < 0.05)].groupby('quarter').size(),\n",
    "            'Negative': self.df[self.df['sentiment'] <= -0.05].groupby('quarter').size()\n",
    "        })\n",
    "        \n",
    "        # Convert to percentages\n",
    "        quarterly_percentages = quarterly_counts.div(quarterly_counts.sum(axis=1), axis=0) * 100\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for column in quarterly_percentages.columns:\n",
    "            plt.plot(range(len(quarterly_percentages)), quarterly_percentages[column], \n",
    "                    marker='o', label=column)\n",
    "        \n",
    "        # Set x-ticks to show only years\n",
    "        years = [str(q).split('Q')[0] for q in quarterly_percentages.index]\n",
    "        unique_years = sorted(list(set(years)))\n",
    "        year_positions = [i for i, year in enumerate(years) if year in unique_years and years.index(year) == i]\n",
    "        \n",
    "        plt.xticks(year_positions, unique_years, rotation=0)\n",
    "        plt.title('Quarterly Sentiment Distribution (%)')\n",
    "        plt.xlabel('Year')\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.ylim(0, 100)  # Set y-axis from 0 to 100%\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.img_dir}/{self.report_prefix}_sentiment_quarterly.png')\n",
    "        plt.close()\n",
    "\n",
    "    def analyze_quarterly_counts(self):\n",
    "\n",
    "        # Group posts by quarter and count\n",
    "        quarterly_counts = self.df.groupby(self.df['timestamp'].dt.to_period('Q')).size()\n",
    "        \n",
    "        # Create visualization\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        quarterly_counts.plot(kind='bar')\n",
    "        \n",
    "        # Customize the plot\n",
    "        plt.title('Quarterly Post Count Distribution')\n",
    "        plt.xlabel('Quarter')\n",
    "        plt.ylabel('Number of Posts')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        \n",
    "        # Add value labels on top of each bar\n",
    "        for i, v in enumerate(quarterly_counts):\n",
    "            plt.text(i, v, str(v), ha='center', va='bottom')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.img_dir}/{self.report_prefix}_quarterly_counts.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Add quarterly counts to report data\n",
    "        self.report_data['quarterly_counts'] = quarterly_counts.to_dict()\n",
    "        \n",
    "        # Print quarterly statistics\n",
    "        print(\"\\nQuarterly Post Counts:\")\n",
    "        for quarter, count in quarterly_counts.items():\n",
    "            print(f\"{quarter}: {count} posts\")\n",
    "\n",
    "    def analyze_yearly_top_keywords(self):\n",
    "\n",
    "        # Add year column to dataframe\n",
    "        self.df['year'] = self.df['timestamp'].dt.year\n",
    "        \n",
    "        # Dictionary to store yearly keywords\n",
    "        yearly_keywords = {}\n",
    "        \n",
    "        # Process each year\n",
    "        for year in sorted(self.df['year'].unique()):\n",
    "            year_data = self.df[self.df['year'] == year]\n",
    "            \n",
    "            # Get all words for this year\n",
    "            all_words = []\n",
    "            for content in year_data['content']:\n",
    "                cleaned_text = self.clean_text(content)\n",
    "                cleaned_words = cleaned_text.split()\n",
    "                all_words.extend(cleaned_words)\n",
    "            \n",
    "            # Count word frequencies for this year\n",
    "            word_freq = Counter(all_words)\n",
    "            \n",
    "            # Store top 3 words\n",
    "            yearly_keywords[year] = dict(word_freq.most_common(3))\n",
    "        \n",
    "        # Store in report data\n",
    "        self.report_data['yearly_top_keywords'] = yearly_keywords\n",
    "        \n",
    "        # Print yearly statistics\n",
    "        print(\"\\nYearly Top 3 Keywords:\")\n",
    "        for year, keywords in yearly_keywords.items():\n",
    "            print(f\"\\n{year}:\")\n",
    "            for word, count in keywords.items():\n",
    "                print(f\"  {word}: {count}\")\n",
    "\n",
    "\n",
    "    def run_analysis(self):\n",
    "\n",
    "        print(f\"Starting analysis of: {self.input_file}\")\n",
    "        \n",
    "        try:\n",
    "            print(\"\\nAnalyzing filtered content...\")\n",
    "            self.analyze_filtered_content()\n",
    "            \n",
    "            print(\"\\nPerforming sentiment analysis...\")\n",
    "            self.perform_sentiment_analysis()\n",
    "            \n",
    "            print(\"\\nAnalyzing engagement patterns...\")\n",
    "            self.analyze_engagement()\n",
    "            \n",
    "            print(\"\\nAnalyzing quarterly post counts...\")\n",
    "            self.analyze_quarterly_counts()\n",
    "            \n",
    "            print(\"\\nAnalyzing yearly top keywords...\")\n",
    "            self.analyze_yearly_top_keywords()\n",
    "            \n",
    "            print(\"\\nSaving report...\")\n",
    "            self.save_report_data()\n",
    "            \n",
    "            print(\"\\nAnalysis complete!\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during analysis: {str(e)}\")\n",
    "            raise\n",
    "            \n",
    "\n",
    "    def get_output_paths(self):\n",
    "\n",
    "        return {\n",
    "            'txt_report':  f'{self.img_dir}/{self.report_prefix}.txt',\n",
    "            'time_series': f'{self.img_dir}/{self.report_prefix}_time_series.png',\n",
    "            'sentiment': f'{self.img_dir}/{self.report_prefix}_sentiment_analysis.png',\n",
    "            'engagement': f'{self.img_dir}/{self.report_prefix}_engagement_distribution.png',\n",
    "            'keyword_freq': f'{self.img_dir}/{self.report_prefix}_keyword_frequency.png'\n",
    "        }\n",
    "    \n",
    "    def analyze_engagement(self):\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.histplot(data=self.df, x='engagement', bins=20)\n",
    "        plt.title('Distribution of Engagement')\n",
    "        plt.xlabel('Engagement Score')\n",
    "        plt.ylabel('Number of Posts')\n",
    "        plt.savefig(self.get_output_paths()['engagement'])\n",
    "        plt.close()\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    def save_report_data(self):\n",
    "\n",
    "        paths = self.get_output_paths()\n",
    "        with open(paths['txt_report'], 'w', encoding='utf-8') as f:\n",
    "            f.write(\"FOMO Analysis Report\\n\")\n",
    "            f.write(\"===================\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Analysis Timestamp: {self.report_data['timestamp']}\\n\")\n",
    "            f.write(f\"Input File: {self.report_data['input_file']}\\n\")\n",
    "            f.write(f\"Total Posts: {self.report_data['total_posts']}\\n\")\n",
    "            f.write(f\"Date Range: {self.report_data['date_range']}\\n\\n\")\n",
    "            \n",
    "            f.write(\"Sentiment Analysis Results\\n\")\n",
    "            f.write(\"------------------------\\n\")\n",
    "            sentiment_data = self.report_data['sentiment_analysis']\n",
    "            f.write(f\"Mean Compound Score: {sentiment_data['mean_compound']:.3f}\\n\")\n",
    "            f.write(f\"Positive Posts: {sentiment_data['positive_posts']} \")\n",
    "            f.write(f\"({sentiment_data['sentiment_distribution']['positive']:.1%})\\n\")\n",
    "            f.write(f\"Neutral Posts: {sentiment_data['neutral_posts']} \")\n",
    "            f.write(f\"({sentiment_data['sentiment_distribution']['neutral']:.1%})\\n\")\n",
    "            f.write(f\"Negative Posts: {sentiment_data['negative_posts']} \")\n",
    "            f.write(f\"({sentiment_data['sentiment_distribution']['negative']:.1%})\\n\\n\")\n",
    "            \n",
    "            if 'engagement_stats' in self.report_data:\n",
    "                f.write(\"Engagement Statistics\\n\")\n",
    "                f.write(\"--------------------\\n\")\n",
    "                for stat, value in self.report_data['engagement_stats'].items():\n",
    "                    f.write(f\"{stat.capitalize()}: {value:.2f}\\n\")\n",
    "                f.write(\"\\n\")\n",
    "            if 'quarterly_counts' in self.report_data:\n",
    "                f.write(\"Quarterly Post Counts\\n\")\n",
    "                f.write(\"-------------------\\n\")\n",
    "                for quarter, count in self.report_data['quarterly_counts'].items():\n",
    "                    f.write(f\"{quarter}: {count} posts\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   # With custom blacklist file\n",
    "    analyzer_with_blacklist = FOMOAnalyzer(\n",
    "        'v3_reddit_fomo_data.csv', \n",
    "        'FOMO_reports', \n",
    "        'v3_reddit_analysis',\n",
    "    )\n",
    "    analyzer_with_blacklist.run_analysis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c199216-9520-404a-b72c-b9e403c1bf73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
